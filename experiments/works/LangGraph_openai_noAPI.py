#!/usr/bin/env python3
"""
Module: 311_wo_LangChain.py
Purpose: Demonstrate token streaming using LangGraph in Python 3.11
         without using LangChain's LLM wrappers.

This module uses an async LLM client (e.g. OpenAI's AsyncOpenAI client)
and a simple LangGraph stateful graph. It streams tokens (from an LLM that
supports streaming) and collects them. The node function returns an update
to the state for the key "joke" (which is annotated to be updated with add_messages).

Requirements:
  - Python 3.11 (for proper context propagation)
  - langgraph package installed
  - openai package installed (for AsyncOpenAI)
  - Proper API key set for OpenAI (set in your environment as OPENAI_API_KEY)
"""
import asyncio
from langgraph.graph import StateGraph, START, END
from typing_extensions import Annotated, TypedDict
from langgraph.graph.message import add_messages
from openai import AsyncOpenAI
import os, httpx

OPENAI_API_BASE_URL = "http://t1cim-wncchat.wneweb.com.tw/v1"
ORION_CTH_API_KEY = os.environ['ORION_CTH_API_KEY'],
http_client = httpx.AsyncClient(verify=False)
# Initialize the async OpenAI client.
# (Ensure your API key is set in your environment: e.g., export OPENAI_API_KEY="YOUR_KEY")
openai_client = AsyncOpenAI(    
    api_key = os.environ['ORION_CTH_API_KEY'],
    base_url = "http://t1cim-wncchat.wneweb.com.tw/v1",
    http_client = http_client 
)


async def stream_tokens(messages):
    """
    Streams tokens from the async LLM client.

    Args:
        messages (list): A list of message dictionaries, each containing keys like "role" and "content".

    Yields:
        str: Each token (as a string) generated by the LLM.
    """
    # Call the async chat completions API with streaming enabled.
    response = await openai_client.chat.completions.create(
        model="gpt-4o-mini-2024-07-18",  # Replace with your desired model
        messages=messages,
        stream=True,
    )
    # Iterate asynchronously over the streaming response.
    async for chunk in response:
        if chunk.choices != []:
            # Each chunk contains a list of choices; we take the first one.
            delta = chunk.choices[0].delta
            # Check if this delta has non-empty "content" (i.e. a token)
            if delta.content:
                # add a 0.5 second delay to simulate real-time streaming
                await asyncio.sleep(0.1)
                yield delta.content
        else:
            # If the chunk has no choices, we yield None.
            yield "\n\n End of stream"


async def call_model(state):
    """
    Node function for the LangGraph. It uses stream_tokens to generate a joke.

    Args:
        state (dict): The state dictionary containing the key "topic" and "joke".

    Returns:
        dict: An update dictionary that writes to the "joke" key.
              Note that since "joke" is annotated with add_messages, its value must be a list.
    """
    topic = state["topic"]
    print("Streaming tokens:")
    collected_tokens = []  # List to collect each token

    # Stream tokens from the async LLM client.
    async for token in stream_tokens(
        [{"role": "user", "content": f"Tell me a joke about {topic}"}]
    ):
        # Print each token immediately, using a pipe '|' as a delimiter.
        print(token, end="|", flush=True)
        collected_tokens.append(token)
    print("\n")  # Newline after streaming is complete

    # Return an update to the state.
    # IMPORTANT: The state is defined so that the key "joke" is expected to be updated with a list.
    # Thus, we wrap the concatenated string in a list.
    full_joke = "".join(collected_tokens)
    return {"joke": [full_joke]}


# Define the state schema using TypedDict.
# This schema specifies that our state includes:
#   - 'topic': a string (the subject of the joke)
#   - 'joke': a list (of messages) that will be updated; annotated with add_messages.
class State(TypedDict):
    topic: str
    joke: Annotated[list, add_messages]


# Build the graph:
# 1. Create a StateGraph using our defined State type.
# 2. Add a single node ("call_model") with our node function.
# 3. Add an edge from the START node to "call_model" and an edge from "call_model" to END.
graph = (
    StateGraph(State)
    .add_node(call_model)
    .add_edge(START, "call_model")
    .add_edge("call_model", END)
    .compile()
)


async def main():
    """
    The main async function:
    - Sets up the initial input state.
    - Runs the graph using astream (asynchronous streaming of updates).
    - Prints the final collected joke if available.
    """
    # Define the input state with a topic.
    inputs = {"topic": "cats", "joke": []}

    # Run the graph asynchronously using astream in "custom" streaming mode.
    async for msg, metadata in graph.astream(inputs, stream_mode="custom"):
        # Check if an update for "joke" is present in the message.
        if msg.get("joke"):
            print("\nFinal joke:", msg["joke"][0])


# This check ensures that main() runs only when the script is executed directly.
if __name__ == "__main__":
    asyncio.run(main())

# works for python 3.10.11