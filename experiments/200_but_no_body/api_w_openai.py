import asyncio
import concurrent.futures
# import openai
from openai import OpenAI
from langgraph.graph import StateGraph, START, END
from fastapi import FastAPI, Query
from fastapi.responses import StreamingResponse
import uvicorn

import os, httpx
OPENAI_API_BASE_URL = "http://t1cim-wncchat.wneweb.com.tw/v1"
ORION_CTH_API_KEY = os.environ['ORION_CTH_API_KEY']
http_client = httpx.Client(verify=False)
# Initialize the async OpenAI client.
# (Ensure your API key is set in your environment: e.g., export OPENAI_API_KEY="YOUR_KEY")
openai = OpenAI(    
    api_key = ORION_CTH_API_KEY,
    base_url = "http://t1cim-wncchat.wneweb.com.tw/v1",
    http_client = http_client 
)


# Define the LangGraph node as an async generator for custom streaming.
async def call_model(state, config):
    """
    A LangGraph node that calls OpenAI's chat.completion.create API with streaming enabled
    and yields tokens as they arrive.
    """
    topic = state["topic"]
    loop = asyncio.get_running_loop()
    q = asyncio.Queue()

    def run_openai():
        try:
            response = openai.chat.completions.create(
                model="gpt-4o-mini-2024-07-18", # works
                # model="gpt-4o-mini",
                # model="gpt-4o-mini-2024-07-18\n",
                messages=[{"role": "user", "content": f"Tell me a joke about {topic}"}],
                stream=True,
            )
            # For each chunk from the API, extract the token and push it onto the queue.
            for chunk in response:
                if chunk.choices != []:
                    token = chunk.choices[0].delta.content
                    if token:
                        asyncio.run_coroutine_threadsafe(q.put(token), loop)
                else:
                    asyncio.run_coroutine_threadsafe(q.put(None), loop)
            # Signal the end of the stream.
            asyncio.run_coroutine_threadsafe(q.put(None), loop)
        except Exception as e:
            asyncio.run_coroutine_threadsafe(q.put(e), loop)
            asyncio.run_coroutine_threadsafe(q.put(None), loop)

    # Run the blocking OpenAI API call in a separate thread.
    with concurrent.futures.ThreadPoolExecutor() as executor:
        executor.submit(run_openai)
        # Instead of aggregating tokens, yield each token as soon as it is available.
        while True:
            token = await q.get()
            if token is None:
                break
            if isinstance(token, Exception):
                raise token
            yield {"token": token}


# Build the LangGraph flow.
graph = (
    StateGraph(dict)
    .add_node(call_model)
    .add_edge(START, "call_model")
    .add_edge("call_model", END)
    .compile()
)

app = FastAPI()


@app.get("/stream")
async def stream_joke(topic: str = Query("dogs", description="Topic for the joke")):
    """
    FastAPI endpoint that streams tokens generated by OpenAI's API.
    The endpoint uses LangGraph in custom streaming mode to send each token as soon as it's produced.
    """

    async def event_generator():
        inputs = {"topic": topic}
        config = {}  # Dummy config if needed.
        # Use "custom" mode so that the node yields tokens one by one.
        async for token_chunk, _ in graph.astream(inputs, stream_mode="custom"):
            yield token_chunk["token"]
            # Optionally add a slight delay.
            await asyncio.sleep(0.01)

    return StreamingResponse(event_generator(), media_type="text/plain")


if __name__ == "__main__":
    uvicorn.run("api_w_openai:app", host="0.0.0.0", port=8000, reload=True)

