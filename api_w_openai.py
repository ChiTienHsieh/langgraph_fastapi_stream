# api_w_openai.py
# This script is a FastAPI endpoint that streams tokens generated by OpenAI's API using LangGraph in custom streaming mode.
# 200 but no response body
import asyncio
from openai import AsyncOpenAI, AuthenticationError, BadRequestError
from langgraph.graph import StateGraph, START, END
from fastapi import FastAPI, Query
from fastapi.responses import StreamingResponse
from langchain_core.runnables import RunnableConfig
import uvicorn
import os
import httpx

# OPENAI_API_BASE_URL = "http://t1cim-wncchat.wneweb.com.tw/v1"
# ORION_CTH_API_KEY = os.environ['ORION_CTH_API_KEY']

# Create an async HTTP client with SSL verification disabled
http_client = httpx.AsyncClient(verify=False)

# Initialize the async OpenAI client
openai_client = AsyncOpenAI(
    # api_key=ORION_CTH_API_KEY,
    # base_url=OPENAI_API_BASE_URL,
    # http_client=http_client
)


async def call_model(state, config: RunnableConfig):
    """
    A LangGraph node that calls OpenAI's chat completion API with streaming enabled
    and yields tokens as they arrive.
    """
    topic = state["topic"]
    try:
        response = await openai_client.chat.completions.create(
            model="gpt-4o-mini-2024-07-18 ",
            messages=[{"role": "user", "content": f"Tell me a joke about {topic}"}],
            stream=True,
            timeout=30.0,
        )

        content_received = False
        async for chunk in response:
            if chunk.choices and chunk.choices[0].delta.content:
                content_received = True
                content = chunk.choices[0].delta.content
                yield {"token": content}

        if not content_received:
            yield {"error": "No content received from API"}

    except AuthenticationError as e:
        yield {"error": f"Authentication error: {str(e)}"}
    
    except BadRequestError as e:
        # print out full e object as json
        print("__dict__\n", e.__dict__)
        import pprint
        print("below is pprint result of vars(e)")
        pprint.pprint(vars(e))
        yield {"error": f"Bad request error: {str(e)}", "status_code": e.status_code, }

    except Exception as e:
        error_msg = f"Connection error: {str(e)}"
        print(f"Error in call_model: {error_msg}")
        yield {"error": error_msg}

"""
response = await openai_client.chat.completions.create(
    model="gpt-4o-mini-2024-07-18 ",
    messages=[{"role": "user", "content": f"Tell me a joke about {topic}"}],
    stream=True,
    timeout=30.0,
)

BadRequestError

below is pprint result of vars(e)
{'body': {'code': None,
          'message': 'invalid model ID',
          'param': None,
          'type': 'invalid_request_error'},
 'code': None,
 'message': "Error code: 400 - {'error': {'message': 'invalid model ID', "
            "'type': 'invalid_request_error', 'param': None, 'code': None}}",
 'param': None,
 'request': <Request('POST', 'https://api.openai.com/v1/chat/completions')>,
 'request_id': 'req_5e0cdcd2f253db14a8b8bf2fdde19456',
 'response': <Response [400 Bad Request]>,
 'status_code': 400,
 'type': 'invalid_request_error'}
 """




# Build the LangGraph flow.
graph = (
    StateGraph(dict)
    .add_node(call_model)
    .add_edge(START, "call_model")
    .add_edge("call_model", END)
    .compile()
)

app = FastAPI()


@app.get("/stream")
async def stream_joke(topic: str = Query("dogs", description="Topic for the joke")):
    """
    FastAPI endpoint that streams tokens generated by OpenAI's API.
    The endpoint uses LangGraph in custom streaming mode to send each token as soon as it's produced.
    """

    async def event_generator():
        inputs = {"topic": topic}
        config = {}
        async for token_chunk, _ in graph.astream(inputs, stream_mode="custom"):
            if "error" in token_chunk:
                yield f"Error: {token_chunk['error']}\n"
                break
            else:
                yield token_chunk["token"]

    return StreamingResponse(event_generator(), media_type="text/plain")


if __name__ == "__main__":
    uvicorn.run("api_w_openai:app", host="0.0.0.0", port=8000, reload=True)
