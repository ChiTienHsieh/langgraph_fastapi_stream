"""
Module: main.py
Purpose: Package the streaming experiment into a FastAPI endpoint.
         This app exposes an endpoint /stream that accepts a 'topic'
         query parameter and streams token-by-token output (a joke)
         from an LLM using LangGraph.

Requirements:
  - Python 3.11 (or 3.10 if configured appropriately)
  - langgraph package installed
  - openai package installed (for AsyncOpenAI)
  - fastapi and uvicorn installed
  - Proper API key set for OpenAI (in environment variable OPENAI_API_KEY)
"""

import asyncio
from fastapi import FastAPI, Query
from fastapi.responses import StreamingResponse
from langgraph.graph import StateGraph, START, END
from typing_extensions import Annotated, TypedDict
from langgraph.graph.message import add_messages
from openai import AsyncOpenAI

# Initialize the async OpenAI client.
openai_client = AsyncOpenAI()


async def stream_tokens(messages):
    """
    Streams tokens from the async LLM client.

    Args:
        messages (list): A list of message dictionaries, e.g.,
                         [{"role": "user", "content": "Tell me a joke about cats"}].

    Yields:
        str: Each token (as a string) generated by the LLM.
    """
    response = await openai_client.chat.completions.create(
        model="gpt-4o-mini",  # Adjust this model name as needed.
        messages=messages,
        stream=True,
    )
    async for chunk in response:
        # Each chunk contains choices; we take the first one.
        delta = chunk.choices[0].delta
        # Check if a token was produced.
        if delta.content:
            yield delta.content + " "


async def call_model(state):
    """
    Node function for LangGraph.
    Streams tokens from the LLM and returns the final joke in the state.

    Args:
        state (dict): The state containing keys "topic" and "joke".

    Returns:
        dict: An update for the state under key "joke". (The value is a list.)
    """
    topic = state["topic"]
    collected_tokens = []  # List to accumulate tokens.

    # Stream tokens from the LLM.
    async for token in stream_tokens(
        [{"role": "user", "content": f"Tell me a joke about {topic}"}]
    ):
        # For local debugging, we print the token (delimited by "|").
        print(token, end="|", flush=True)
        collected_tokens.append(token)
    print("\n")  # Newline after streaming is complete.

    # Concatenate tokens and return as a list (to match the state annotation).
    full_joke = "".join(collected_tokens)
    return {"joke": [full_joke]}


# Define the state schema using TypedDict.
# The state includes:
#   - "topic": a string specifying the joke topic.
#   - "joke": a list that will be updated (annotated with add_messages).
class State(TypedDict):
    topic: str
    joke: Annotated[list, add_messages]


# Build the LangGraph:
# 1. Create a StateGraph using our defined State type.
# 2. Add the node "call_model".
# 3. Connect START -> call_model -> END.
graph = (
    StateGraph(State)
    .add_node(call_model)
    .add_edge(START, "call_model")
    .add_edge("call_model", END)
    .compile()
)

# Create the FastAPI app.
app = FastAPI(title="LangGraph Streaming API")


async def token_generator(topic: str):
    # Set up the initial state.
    inputs = {"topic": topic, "joke": []}
    # Use astream_events to capture internal events from the graph.
    # Version "v1" is used here; adjust if necessary.
    async for msg, _ in graph.astream(inputs, stream_mode="custom"):
        if msg.get("joke"):
            joke_text = msg["joke"][0]
            for token in joke_text.split():
                yield (token + " ").encode("utf-8")
                await asyncio.sleep(0.0)  # Ensures immediate streaming


@app.get("/stream", response_class=StreamingResponse)
async def stream_joke(topic: str = Query(..., description="Topic for the joke")):
    """
    FastAPI endpoint to stream the LLM's token output.

    Query Parameter:
      - topic: The subject for the joke.

    Returns:
      StreamingResponse: A text/plain stream of tokens.
    """

    return StreamingResponse(
        token_generator(topic),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Transfer-Encoding": "chunked",
        },
    )


# If this module is run directly, start the FastAPI server with uvicorn.
if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000)
