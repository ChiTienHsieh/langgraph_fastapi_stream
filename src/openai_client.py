# src/openai_client.py
import os
import asyncio
import httpx
from typing import AsyncGenerator
from openai import AsyncOpenAI  # Ensure you have an async OpenAI client installed

# Configuration for the async OpenAI client.
OPENAI_API_BASE_URL = "http://t1cim-wncchat.wneweb.com.tw/v1"
ORION_CTH_API_KEY = os.environ['ORION_CTH_API_KEY']

# Create an async HTTP client. Disabling SSL verification as per your snippet.
http_client = httpx.AsyncClient(verify=False)

# Initialize the async OpenAI client.
openai_client = AsyncOpenAI(
    api_key=ORION_CTH_API_KEY,
    base_url=OPENAI_API_BASE_URL,
    http_client=http_client
)

async def stream_openai_tokens(topic: str) -> AsyncGenerator[str, None]:
    """
    Streams tokens from the async LLM client.

    Args:
        topic (str): The topic for the joke.

    Yields:
        str: Each token (as a string) generated by the LLM.
    """
    messages = [{"role": "user", "content": f"Tell me a joke about {topic}"}]
    # Call the async chat completions API with streaming enabled.
    response = await openai_client.chat.completions.create(
        model="gpt-4o-mini-2024-07-18",  # Replace with your desired model
        messages=messages,
        stream=True,
    )
    # Iterate asynchronously over the streaming response.
    async for chunk in response:
        if chunk.choices != []:
            # Each chunk contains a list of choices; we take the first one.
            delta = chunk.choices[0].delta
            # Check if this delta has non-empty "content" (i.e. a token)
            if delta.content:
                # Add a short delay to simulate real-time streaming.
                await asyncio.sleep(0.1)
                yield delta.content
        else:
            # If the chunk has no choices, yield an end-of-stream marker.
            yield "\n\n End of stream"
