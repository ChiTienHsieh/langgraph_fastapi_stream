# src/langchain_openai_client.py
import asyncio
import os
from typing import AsyncGenerator
from langchain_openai import ChatOpenAI
from langchain.callbacks.base import BaseCallbackHandler
from langchain_core.runnables import RunnableConfig

class QueueCallbackHandler(BaseCallbackHandler):
    """Callback handler that pushes tokens to an asyncio queue."""
    def __init__(self, queue: asyncio.Queue):
        self.queue = queue

    async def on_llm_new_token(self, token: str, **kwargs):
        # Push the token to the queue.
        await self.queue.put(token)
        # Increase delay to simulate real-time streaming.
        await asyncio.sleep(0.2)  # Adjust delay as needed

async def stream_langchain_tokens(topic: str, config: RunnableConfig = None) -> AsyncGenerator[str, None]:
    """
    Streams tokens using LangChain's ChatOpenAI client with streaming enabled.
    Tokens are yielded as they are received via the callback.

    Args:
        topic (str): The topic for the joke.

    Yields:
        str: Each token (as a string) generated by the LLM.

    Raises:
        Exception: If there's an error during API communication or token streaming.
    """
    queue = asyncio.Queue()
    handler = QueueCallbackHandler(queue)
    content_received = False
    error = None

    try:
        # Initialize the model with streaming enabled and our callback.
        model = ChatOpenAI(
            model="gpt-4o-mini-2024-07-18",
            streaming=True,
            callbacks=[handler],
            request_timeout=30.0,  # Add timeout for API calls
            # base_url="http://t1cim-wncchat.wneweb.com.tw/v1", 
            # api_key=os.environ['ORION_CTH_API_KEY']
        )

        # Run the model invocation in the background with config
        generate_task = asyncio.create_task(
            model.ainvoke(
                [{"role": "user", "content": f"Tell me a joke about {topic}"}],
                config=config
            )
        )

        # Create a background task that waits for the generation to complete,
        # then pushes a sentinel value (None) into the queue.
        async def put_sentinel():
            try:
                await generate_task
                await queue.put(None)
            except Exception as e:
                nonlocal error
                error = e
                await queue.put(None)  # Still send sentinel to unblock consumer

        sentinel_task = asyncio.create_task(put_sentinel())

        # Consume tokens from the queue as they arrive.
        while True:
            try:
                # Add timeout to queue.get to prevent hanging
                token = await asyncio.wait_for(queue.get(), timeout=30.0)
                if token is None:
                    break
                content_received = True
                # Optionally add a small delay in the consumer loop.
                await asyncio.sleep(0.1)
                yield token
            except asyncio.TimeoutError:
                raise Exception("Timeout waiting for tokens")

        # If there was an error in the background task, raise it
        if error:
            raise error

        # If we haven't received any content, raise an error
        if not content_received:
            raise Exception("No content received from API")

        # Always yield end marker after successful streaming
        yield "\n\n End of stream"

    except Exception as e:
        # Log the error and re-raise
        print(f"Error in stream_langchain_tokens: {str(e)}")
        raise

    finally:
        # Clean up tasks if they're still running
        if 'generate_task' in locals():
            generate_task.cancel()
        if 'sentinel_task' in locals():
            sentinel_task.cancel()
